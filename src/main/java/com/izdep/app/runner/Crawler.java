package com.izdep.app.runner;

import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.safety.Whitelist;
import org.jsoup.select.Elements;

import java.io.*;
import java.net.HttpURLConnection;
import java.net.URL;
import java.sql.*;
import java.util.ArrayList;
import java.util.Properties;

public class Crawler implements Runnable {
    public final static String TABLE_URLS = "URLS";
    public final static String TABLE_IMAGES = "IMAGES";
    public final static String TABLE_WORD = "WORD";
    public final static String TABLE_IMGWORD = "IMGWORD";
    public final static int nThread = 1;

    private Connection connection;
    private int NextURLID, NextImageURLID, NextURLIDScanned, urlIndex, MaxURLs;
    private String domain;
    private ArrayList<String> urlList;
    private Properties props;
    private boolean reset = true;

    private final Object monitor = new Object();

    public Crawler(int MaxURLs, String domain, boolean reset, ArrayList<String> urlList) throws IOException {
        readProperties();
        setVariables();
        this.MaxURLs = MaxURLs;
        this.domain = domain;
        this.urlList = urlList;
        this.reset = reset;
    }

    public Crawler(Connection connection) {
        this.connection = connection;
    }

    public void run() {
        try {
            crawl();
        } catch (Exception e) {
        }
    }

    /**
     * Open the properties file to read necessary data such as database
     * information and counters
     *
     * @throws IOException
     */
    public void readProperties() throws IOException {
        props = new Properties();
        FileInputStream in = new FileInputStream("src/main/resources/application.properties");
        props.load(in);
        in.close();
    }

    /**
     * Save constantly changing counters to properties file to save crawler state
     *
     * @throws IOException
     */
    public synchronized void setProperties() throws IOException {
        props.setProperty("crawler.NextURLID", "" + NextURLID);
        props.setProperty("crawler.NextURLIDScanned", "" + NextURLIDScanned);
        props.setProperty("crawler.NextImageURLID", "" + NextImageURLID);
        FileOutputStream out = new FileOutputStream("src/main/resources/application.properties");
        props.store(out, null);
    }

    /**
     * Get data stored in properties file and save them to the global variables
     */
    public synchronized void setVariables() {
        NextURLID = Integer.parseInt(props.getProperty("crawler.NextURLID"));
        NextURLIDScanned = Integer.parseInt(props.getProperty("crawler.NextURLIDScanned"));
        NextImageURLID = Integer.parseInt(props.getProperty("crawler.NextImageURLID"));
    }

    /**
     * Create connection the the database using DB url, username, and password
     * stored in the properties file
     *
     * @throws SQLException
     * @throws IOException
     */
    public void openConnection() throws SQLException, IOException {
        String drivers = props.getProperty("jdbc.drivers");
        if (drivers != null)
            System.setProperty("jdbc.drivers", drivers);

        String url = props.getProperty("jdbc.url");
        String username = props.getProperty("jdbc.username");
        String password = props.getProperty("jdbc.password");

        connection = DriverManager.getConnection(url, username, password);
    }

    /**
     * Drop all previously created tables in the DB that the crawler will use
     * and create new ones.
     *
     * @throws SQLException
     * @throws IOException
     */
    public void createDB() throws SQLException, IOException {
        System.out.println("Connecting to Database...");
        openConnection();

        Statement stat = connection.createStatement();

        // Delete the tables first if any
        try {
            System.out.println("Removing Previously Created Tables...");
            stat.executeUpdate("DROP TABLE URLS");
            stat.executeUpdate("DROP TABLE IMAGES");
            stat.executeUpdate("DROP TABLE WORD");
            stat.executeUpdate("DROP TABLE IMGWORD");
        } catch (Exception e) {
        }

        // Create the tables
        System.out.println("Creating Tables...");
        stat.executeUpdate("CREATE TABLE URLS (urlid INT, " +
                "url VARCHAR(20000) CHARACTER SET utf8 COLLATE utf8_general_ci, " +
                "rank INT, " +
                "title VARCHAR(200) CHARACTER SET utf8 COLLATE utf8_general_ci, " +
                "description VARCHAR(200) CHARACTER SET utf8 COLLATE utf8_general_ci)");
        stat.executeUpdate("CREATE TABLE IMAGES (urlid INT, " +
                "url VARCHAR(20000) CHARACTER SET utf8 COLLATE utf8_general_ci, " +
                "rank INT)");
        stat.executeUpdate("CREATE TABLE WORD (word VARCHAR(2000), " +
                "urllist VARCHAR(18000)CHARACTER SET utf8 COLLATE utf8_general_ci)");
        stat.executeUpdate("CREATE TABLE IMGWORD (word VARCHAR(2000) CHARACTER SET utf8 COLLATE utf8_general_ci, " +
                "urllist VARCHAR(18000) CHARACTER SET utf8 COLLATE utf8_general_ci)");
        stat.close();
    }

    /**
     * Check if urlFound is already in the given table
     *
     * @param urlFound
     * @param table
     * @return
     * @throws SQLException
     * @throws IOException
     */
    public synchronized boolean urlInDB(String urlFound, String table) throws SQLException, IOException {
        PreparedStatement pstmt = connection
                .prepareStatement("SELECT * FROM " + table + " WHERE url LIKE ?");
        pstmt.setString(1, urlFound);
        ResultSet result = pstmt.executeQuery();

        if (result.next()) {
            //System.out.println("URL " + urlFound + " already in DB");
            pstmt.close();
            return true;
        }
        // System.out.println("URL "+urlFound+" not yet in DB");
        pstmt.close();
        return false;
    }

    /**
     * Insert a url into the given table with the specified urlid
     *
     * @param url
     * @param urlid
     * @param table
     * @throws SQLException
     * @throws IOException
     */
    public synchronized void insertURLInDB(String url, int urlid, String table)
            throws SQLException, IOException {
        PreparedStatement pstmt;
        if (table.equals(TABLE_URLS))
            pstmt = connection.prepareStatement("INSERT INTO " + table
                    + " VALUES (?, ?, 1, '', '')");
        else
            pstmt = connection.prepareStatement("INSERT INTO " + table
                    + " VALUES (?, ?, 1)");
        pstmt.setInt(1, urlid);
        pstmt.setString(2, url);
        pstmt.executeUpdate();
        pstmt.close();
    }

    /**
     * Check if a word is found in the specified table
     *
     * @param word
     * @param table
     * @return
     * @throws SQLException
     * @throws IOException
     */
    public synchronized boolean wordInDB(String word, String table) throws SQLException, IOException {
        PreparedStatement pstmt = connection
                .prepareStatement("SELECT * FROM " + table + " WHERE word LIKE ?");
        pstmt.setString(1, word);
        ResultSet result = pstmt.executeQuery();

        if (result.next()) {
            //System.out.println("WORD " + word + " already in DB");
            pstmt.close();
            return true;
        }
        // System.out.println("URL "+urlFound+" not yet in DB");
        pstmt.close();
        return false;
    }

    /**
     * Insert a new word into the given table with the specified url list
     *
     * @param word
     * @param urllist
     * @param table
     * @throws SQLException
     * @throws IOException
     */
    public synchronized void insertWordInDB(String word, String urllist, String table) throws SQLException, IOException {
        PreparedStatement pstmt = connection
                .prepareStatement("INSERT INTO " + table + " VALUES (?, ?)");
        pstmt.setString(1, word);
        pstmt.setString(2, urllist);
        pstmt.executeUpdate();
        pstmt.close();
    }

    /**
     * Get the url list of a word from the specified table
     *
     * @param word
     * @param table
     * @return urllist
     * @throws SQLException
     * @throws IOException
     */
    public synchronized String getURLListFromDB(String word, String table) throws SQLException, IOException {
        PreparedStatement pstmt = connection
                .prepareStatement("SELECT * FROM " + table + " WHERE word LIKE ?");
        pstmt.setString(1, word);
        ResultSet result = pstmt.executeQuery();
        result.next();
        //System.out.println("WORD " + word + ": " + result.getString(2));
        String list = result.getString(2);
        pstmt.close();
        return list;
    }

    /**
     * Get the page rank of the given url in the specified table
     *
     * @param url
     * @param table
     * @return rank
     * @throws SQLException
     * @throws IOException
     */
    public synchronized int getURLRankFromDB(String url, String table) throws SQLException, IOException {
        PreparedStatement pstmt = connection
                .prepareStatement("SELECT * FROM " + table + " WHERE url LIKE ?");
        pstmt.setString(1, url);
        ResultSet result = pstmt.executeQuery();
        result.next();
        //System.out.println("WORD " + word + ": " + result.getString(2));
        int rank = result.getInt(3);
        pstmt.close();
        return rank;
    }

    /**
     * Check to see if the given url actually exists and is found on the web.
     * This is an important check so the html parser is not given a url that is
     * no longer available or found.
     *
     * @param link
     * @return
     */
    public boolean isHTML(String link) {
        URL url;
        HttpURLConnection urlc = null;
        try {
            url = new URL(link);
            urlc = (HttpURLConnection) url.openConnection();
            urlc.setAllowUserInteraction(false);
            urlc.setDoInput(true);
            urlc.setDoOutput(false);
            urlc.setUseCaches(true);

            // Only get the head of the document to save space and time
            urlc.setRequestMethod("HEAD");
            urlc.connect();

            // Check the content type to make sure the document is HTML
            String mime = urlc.getContentType();
            if (mime.contains("text/html")) {
                return true;
            }
        } catch (Exception e) {
            //e.printStackTrace();
        } finally {
            if (urlc != null)
                urlc.disconnect();
        }

        return false;
    }

    /**
     * Check to see that an image actually exists given a url and has not been
     * removed.
     *
     * @param url
     * @return
     */
    public boolean imageExists(String url) {

        HttpURLConnection con = null;

        try {
            HttpURLConnection.setFollowRedirects(false);

            // Only get the head of the document to save space and time
            con =
                    (HttpURLConnection) new URL(url).openConnection();
            con.setRequestMethod("HEAD");
            return (con.getResponseCode() == HttpURLConnection.HTTP_OK);
        } catch (Exception e) {
            //e.printStackTrace();
            return false;
        } finally {
            if (con != null)
                con.disconnect();
        }
    }

    /**
     * Get the contents of a specified meta tag. This is used to get the
     * document description in the main crawler method.
     *
     * @param document
     * @param attr
     * @return
     */
    public String getMetaTag(Document document, String attr) {
        // Try to see if there is content in the name variable
        Elements elements = document.select("meta[name=" + attr + "]");
        for (Element element : elements) {
            final String s = element.attr("content");
            if (s != null)
                return s;
        }

        // Try to see if there is content in the property variable
        elements = document.select("meta[property=" + attr + "]");
        for (Element element : elements) {
            final String s = element.attr("content");
            if (s != null)
                return s;
        }
        return null;
    }

    /**
     * An initializing method for the crawl. Opens a database connection, creates
     * the DB and sets the necessary variables to their appropriate values.
     *
     * @throws SQLException
     * @throws IOException
     */
    public void startCrawl() throws SQLException, IOException {
        /* If the reset flag is not on, no need to recreate the DB. Just open the
         * connection. If there are no urls scanned, reset database anyways.
         */
        if (!reset && NextURLIDScanned > 0) {
            openConnection();
            return;
        }

        createDB();

        // Reset variables
        NextURLID = 0;
        NextURLIDScanned = 0;
        NextImageURLID = 0;

        /* For each url in the entered urlList from either the properties file or
         * command arguments, add them to the database and increment the counter
         */
        int urlID;
        for (String url : urlList) {
            urlID = NextURLID;
            insertURLInDB(url, urlID, TABLE_URLS);
            NextURLID++;
        }

        // Save the counters to the properties file
        setProperties();

        return;

    }

    /**
     * Get the next url from the specified table to be parsed
     *
     * @param table
     * @return url
     * @throws SQLException
     * @throws IOException
     */
    public synchronized String getNextURL(String table) throws SQLException, IOException {
        // Get the variables from the properties file
        setVariables();

        // Create a query to get the required row
        urlIndex = NextURLIDScanned;
        Statement stat = connection.createStatement();
        ResultSet result = stat
                .executeQuery("SELECT * FROM " + table + " WHERE urlid = " + urlIndex);
        result.next();

        // Get the url from the url column
        String url1 = result.getString(2);
        stat.close();

        NextURLIDScanned++;

        // Save the counters to the properties file
        setProperties();

        return url1;
    }

    /**
     * Open connection to url1 and create a DOM document from the Jsoup parser.
     *
     * @param url1
     * @return document
     * @throws IOException
     */
    public synchronized Document parseURL(String url1) throws IOException {
        InputStream in = null;
        Document doc = null;
        try {
            System.out.println("[" + Thread.currentThread().getName() + "] [" + urlIndex + "] " + url1);
            in = new URL(url1).openStream();
            doc = Jsoup.parse(in, "UTF-8", url1);
        } catch (Exception e) {
            //e.printStackTrace();
            System.out.println("Could Not Obtain Document...");
            System.out.println("Skipped.");
            System.out.println("-------------------------------------------");
            doc = null;
        } finally {
            if (in != null)
                in.close();
        }

        return doc;
    }

    /**
     * Extract all href links from the given document and add them to the
     * database
     *
     * @param doc
     * @throws SQLException
     * @throws IOException
     */
    public synchronized void extractDocumentURLs(Document doc)
            throws SQLException, IOException {
        Elements links;
        PreparedStatement pstmt;

        System.out.println("Extracting URLs...");
        links = doc.select("a");

        for (Element e : links) {
            // Get the absolute url
            String urlFound = e.attr("abs:href");
            urlFound = urlFound.trim();
            boolean found = urlInDB(urlFound, TABLE_URLS);

            if (found) {
                // Increment the current url's rank and update its value in the DB
                int rank = getURLRankFromDB(urlFound, TABLE_URLS);
                rank++;

                // Increment rank in url found
                pstmt = connection
                        .prepareStatement("UPDATE urls SET rank = ? WHERE url = ?");
                pstmt.setInt(1, rank);
                pstmt.setString(2, urlFound);
                pstmt.executeUpdate();
                pstmt.close();
            }

            /* For a url to be added to the DB it must be http, contain the domain,
             * and must be an HTML document
             */
            if (!found && NextURLIDScanned < nThread && urlFound.contains("http://")
                    && urlFound.contains(domain) && !urlFound.contains("#")
                    && isHTML(urlFound)) {
                insertURLInDB(urlFound, NextURLID, TABLE_URLS);
                NextURLID++;

                // Save the counters to the properties file
                setProperties();
            }
        }

    }

    /**
     * Extract all images from the given document
     *
     * @param doc
     * @throws SQLException
     * @throws IOException
     */
    public synchronized void extractDocumentImages(Document doc)
            throws SQLException, IOException {
        Elements images;
        PreparedStatement pstmt;

        System.out.println("Extracting Images...");
        images = doc.select("img");

        for (Element e : images) {
            // Get the absolute image url
            String imageFound = e.attr("abs:src");
            imageFound = imageFound.trim();
            boolean found = urlInDB(imageFound, TABLE_IMAGES);

            if (found) {
                // Increment the current url's rank and update its value in the DB
                int rank = getURLRankFromDB(imageFound, TABLE_IMAGES);
                rank++;

                // Increment rank in url found
                pstmt = connection
                        .prepareStatement("UPDATE images SET rank = ? WHERE url = ?");
                pstmt.setInt(1, rank);
                pstmt.setString(2, imageFound);
                pstmt.executeUpdate();
                pstmt.close();
            }
            /* For an image to be added to the DB it must be http and must exist
             */
            if (!found && imageFound.contains("http://") && imageExists(imageFound)) {
                insertURLInDB(imageFound, NextImageURLID, TABLE_IMAGES);
                NextImageURLID++;

                // Save the counters to the properties file
                setProperties();
            }
        }
    }

    /**
     * The main crawl method. This is where all the magic happens.
     * Uses Breadth-First Searching algorithm to get urls and images, parse
     * documents, and populate the database
     *
     * @throws SQLException
     * @throws IOException
     * @throws InterruptedException
     */
    public void crawl() throws SQLException, IOException, InterruptedException {

        Document doc;
        String text, urllist;
        int imgIndexStart = 0;
        int imgIndexEnd = 0;
        String[] words;
        PreparedStatement pstmt;
        setVariables();

        // Let the first thread do some work before starting the rest of the threads.
        if (!Thread.currentThread().getName().equals("0")) {
            while (NextURLIDScanned < nThread)
                Thread.sleep(100);
        }


        while (NextURLIDScanned < NextURLID) {
            // Get the next url to be parsed
            String url1 = getNextURL(TABLE_URLS);
            doc = parseURL(url1);
            if (doc == null)
                continue;

            synchronized (monitor) {
                // Get title
                String title = doc.title();

                /* Get Description by either the description meta tag, first 100
                 * characters of the body or just the first 100 characters
                 */
                String description = getMetaTag(doc, "description");
                if (description == null && doc.body() != null)
                    description = doc.body().text();
                else if (description == null)
                    description = doc.text();
                if (description.length() > 100)
                    description = description.substring(0, 100);

                System.out.println(description);

                // Update database with description and title
                pstmt = connection
                        .prepareStatement("UPDATE urls SET title = ?, description = ? WHERE url like ?");
                pstmt.setString(1, title);
                pstmt.setString(2, description);
                pstmt.setString(3, url1);
                pstmt.executeUpdate();
                pstmt.close();

                // Custom checks for cs.purdue.edu
                if (url1.contains("#") || url1.contains("&oldid=") || url1.contains("calendar/webevent.cgi?"))
                    continue;
            }

            // Get all link elements
            if (NextURLID < MaxURLs) {
                extractDocumentURLs(doc);
            }

            /* Get the start and end indices to get the total number of images in
             * the document
             */
            imgIndexStart = NextImageURLID;
            extractDocumentImages(doc);
            imgIndexEnd = NextImageURLID;

            // Get document parsed text (no tags)
            System.out.println("Parsing Document...");
            // Remove html white spaces
            doc.select(":containsOwn(\u00a0)").remove();
            text = doc.text();
            // Remove remaining HTML code
            text = Jsoup.clean(text, Whitelist.relaxed());

            // Get each word of the document
            words = text.split("\\s+"); // split the string by white spaces to get individual words
            for (String word : words) {
                word = word.toLowerCase(); // Lower case all words
                word = word.replaceAll("[^A-Za-z0-9]", ""); // Remove punctuation
                if (word.matches("[a-zA-Z0-9]+")) // If the word is letters and numbers only
                {
                    // Add words to urls table
                    if (!wordInDB(word, TABLE_WORD)) {
                        // If the word is not in the table, create a new entry
                        insertWordInDB(word, urlIndex + "", TABLE_WORD);
                    } else {
                        // Else, update the current entry with the current urlIndex
                        urllist = getURLListFromDB(word, TABLE_WORD);
                        if (!urllist.contains(String.valueOf(urlIndex))) // Check to not add duplicates
                            urllist += "," + urlIndex;
                        synchronized (monitor) {
                            pstmt = connection
                                    .prepareStatement("UPDATE word SET urllist = ? WHERE word = ?");
                            pstmt.setString(1, urllist);
                            pstmt.setString(2, word);
                            pstmt.executeUpdate();
                            pstmt.close();
                        }
                    }

                    // Add words to image table
                    if (!wordInDB(word, TABLE_IMGWORD)) {
                        if (imgIndexStart != imgIndexEnd) {
                            /*
                             * Since every document can have multiple images, each word
                             * must contain all image urlids found in that document
                             */
                            urllist = imgIndexStart + "";
                            for (int i = imgIndexStart + 1; i < imgIndexEnd; i++)
                                urllist = urllist + "," + i;
                            insertWordInDB(word, urllist, TABLE_IMGWORD);
                        }
                    } else {
                        /* add the words to the imgWord table as well for each image
                         * found in the current doc
                         */
                        urllist = getURLListFromDB(word, TABLE_IMGWORD);
                        for (int i = imgIndexStart + 1; i < imgIndexEnd; i++) {
                            if (!urllist.contains(String.valueOf(i))) // Check to not add duplicates
                                urllist += "," + i;
                        }
                        synchronized (monitor) {
                            pstmt = connection
                                    .prepareStatement("UPDATE imgword SET urllist = ? WHERE word = ?");
                            pstmt.setString(1, urllist);
                            pstmt.setString(2, word);
                            pstmt.executeUpdate();
                            pstmt.close();
                        }
                    }
                }
            }

            System.out.println("Done.");
            System.out.println("-------------------------------------------");

            setProperties();
        }
    }

    /**
     * Create the Crawler object with initial command line arguments
     *
     * @param args
     * @return
     * @throws IOException
     */
    public static Crawler crawlerWithArguments(String[] args) throws IOException {
        // Default values
        String domain = "cs.purdue.edu";
        String root = "http://www.cs.purdue.edu/";
        int MaxURLs = 1000;
        boolean reset = false;
        ArrayList<String> urlList = new ArrayList<String>();

        // Parse arguments
        for (int i = 0; i < args.length; i++) {
            if (args[i].equals("-u")) // Max URLs
            {
                MaxURLs = Integer.parseInt(args[++i]);
            } else if (args[i].equals("-d")) // Domain
            {
                domain = args[++i];
            } else if (args[i].equals("-r")) // Reset flag
            {
                reset = true;
            } else {
                urlList.add(args[i]); // URL List
            }
        }

        if (urlList.size() == 0)
            urlList.add(root);

        System.out.println("ARGS: webcrawl -u " + MaxURLs + " -d " + domain + " -r " + reset + " " + urlList);
        Crawler crawler = new Crawler(MaxURLs, domain, reset, urlList);

        return crawler;
    }

    /**
     * Create the Crawler object with the properties file if available
     *
     * @return
     * @throws IOException
     */
    public static Crawler crawlerWithProperties() throws IOException {
        // Open the properties file
        Properties props = new Properties();
        FileInputStream in = new FileInputStream("src/main/resources/application.properties");
        props.load(in);
        in.close();

        // Get the necessary arguments
        String domain = props.getProperty("crawler.domain");
        String root = props.getProperty("crawler.root");
        int MaxURLs = Integer.parseInt(props.getProperty("crawler.maxurls"));
        boolean reset;
        if (props.getProperty("crawler.reset").equals("YES"))
            reset = true;
        else
            reset = false;
        ArrayList<String> urlList = new ArrayList<String>();
        urlList.add(root);

        // Create the crawler
        System.out.println("PROPS: webcrawl -u " + MaxURLs + " -d " + domain + " -r " + reset + " " + urlList);
        Crawler crawler = new Crawler(MaxURLs, domain, reset, urlList);

        return crawler;

    }

    /**
     * Attempt to create a Crawler from either the properties file, command line
     * arguments, or default values. Which ever one is available.
     *
     * @param args
     * @return
     * @throws IOException
     */
    public static Crawler createCrawler(String[] args) throws IOException {
        // Default Values
        String domain = "cs.purdue.edu";
        String root = "http://www.cs.purdue.edu/";
        int MaxURLs = 1000;
        boolean reset = true;
        ArrayList<String> urlList = new ArrayList<String>();

        File propertiesFile = new File("src/main/resources/application.properties");
        Crawler crawler;

        // Get the necessary variables from args, props file, or defaults
        // Arguments given
        if (args.length > 0)
            crawler = crawlerWithArguments(args);
            // No arguments given but properties file is available
        else if (args.length == 0 || propertiesFile.exists())
            crawler = crawlerWithProperties();
            // Default values
        else {
            urlList.add(root);
            System.out.println("DEFAULT: webcrawl -u " + MaxURLs + " -d " + " -r " + reset + " " + domain + " " + urlList);
            crawler = new Crawler(MaxURLs, domain, reset, urlList);
        }

        return crawler;
    }

    public static void main(String[] args) throws IOException {
        // Create the crawler
        Crawler crawler = createCrawler(args);

        //Create the threads
        Thread threads[] = new Thread[nThread];
        for (int i = 0; i < threads.length; i++)
            threads[i] = new Thread(crawler, "" + i);

        // Crawl that sucker
        try {
            System.out.println("Initializing Crawl Sequence...");
            crawler.startCrawl();
            System.out.println("Crawling...");
            System.out.println("-------------------------------------------");
            for (int i = 0; i < threads.length; i++)
                threads[i].start();

            for (int i = 0; i < threads.length; i++)
                threads[i].join();
        } catch (Exception e) {
            e.printStackTrace();
        }
        System.out.println("Crawling Complete.");
    }

}
